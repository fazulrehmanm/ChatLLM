{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22b4b44",
   "metadata": {},
   "source": [
    "\n",
    "# üèÅ SegFormer Fine-tuning for Semantic Segmentation\n",
    "\n",
    "In this notebook, we demonstrate how to fine-tune **SegFormer**, a Vision Transformer-based segmentation model using PyTorch and Hugging Face.\n",
    "\n",
    "We'll:\n",
    "- Load an image segmentation dataset from Hugging Face (`datasets`)\n",
    "- Preprocess using `SegformerImageProcessor`\n",
    "- Train a SegFormer model\n",
    "- Evaluate using mean IoU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa69f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q transformers datasets evaluate torch torchvision timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff5837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "from transformers import AdamW, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f965b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, hf_ds, processor):\n",
    "        self.ds = hf_ds\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        image = Image.fromarray(item['image'])\n",
    "        mask = Image.fromarray(item['segmentation_mask'])\n",
    "        encoded = self.processor(image, mask, return_tensors=\"pt\")\n",
    "        return {k: v.squeeze() for k, v in encoded.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fbe969",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace 'scene_parse_150' with your custom dataset if needed\n",
    "dataset = load_dataset(\"scene_parse_150\")\n",
    "train_ds = dataset[\"train\"].shuffle(seed=42).select(range(100))  # subset for demo\n",
    "val_ds = dataset[\"validation\"].select(range(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ab0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "processor = SegformerImageProcessor.from_pretrained(checkpoint)\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(checkpoint)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72805a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(SegmentationDataset(train_ds, processor), batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(SegmentationDataset(val_ds, processor), batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87240f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 5\n",
    "num_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f21bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ef340",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    for batch in val_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        preds = outputs.logits.argmax(dim=1).cpu().numpy()\n",
    "        labels = batch[\"labels\"].cpu().numpy()\n",
    "        metric.add_batch(predictions=preds, references=labels)\n",
    "\n",
    "    results = metric.compute(num_labels=model.config.num_labels, ignore_index=255, reduce_labels=False)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} ‚Äî mIoU: {results['mean_iou']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}